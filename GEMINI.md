# nasim環境における強化学習を用いたサイバー攻撃・防御の研究

## 1. 概要

このリポジトリは、サイバーセキュリティのシミュレーション環境である[nasim](https://github.com/lmz84/nasim)を利用して、強化学習（RL）エージェントによるサイバー攻撃および防御戦略を研究するためのものです。

目的は、エージェントがネットワーク内を探索し、脆弱性を発見・悪用して目的のサーバー（ターゲット）を攻略する過程をモデル化し、最適な攻撃経路や防御策を分析することです。

## 2. nasim環境について

`nasim`は、ネットワーク侵入テストをシミュレートするために設計された環境です。以下のような特徴があります。

-   **構成可能なネットワーク**: ホスト、サブネット、脆弱性、ファイアウォールルールなどを定義したシナリオファイル（`.yaml`）を基に、多様なネットワーク環境を構築できます。
-   **現実的なアクション**: ポートスキャン、権限昇格、脆弱性攻撃など、ペネトレーションテストにおける現実的なアクションが定義されています。
-   **観測情報**: エージェントは、完全に観測可能な状態（`fully_obs=True`）または部分的に観測可能な状態（`fully_obs=False`）で環境を認識します。このリポジトリでは、学習を容易にするため完全観測モードを採用しています。

## 3. 強化学習アプローチ

本研究では、基本的な強化学習アルゴリズムである**Q学習**を実装しています（`rl_1.py`）。

### Q学習の実装 (`rl_1.py`)

-   **Qテーブル**: `(状態, 行動)`のペアに対応するQ値を保存する辞書（`q_table`）を使用します。
    -   **状態**: `nasim`環境の状態（`state`）はNumPy配列で表現されるため、辞書のキーとして利用できるようタプルに変換しています。
    -   **行動**: エージェントが取れる行動は整数で表現されます（`flat_actions=True`）。
-   **学習プロセス**:
    1.  **行動選択**: ε-Greedy法により、「探索（ランダムな行動）」と「活用（Q値が最大の行動）」をバランス良く行います。
    2.  **Q値の更新**: エージェントが行動を実行して得られた「報酬（`reward`）」と「次の状態（`next_state`）」を基に、以下のベルマン方程式を用いてQ値を更新します。
        ```
        Q(s, a) ← Q(s, a) + α * [r + γ * max_a' Q(s', a') - Q(s, a)]
        ```
        -   `α (ALPHA)`: 学習率
        -   `γ (GAMMA)`: 割引率
-   **学習と評価**:
    -   `EPISODES`で指定された回数のエピソードを繰り返し、Qテーブルを学習します。
    -   学習終了後、ε-Greedy法を使わずに、学習したQテーブルに基づいて最適な行動を取り続けるテストプレイを実行し、成果を確認します。

## 4. コード構成

-   `rl_1.py`: Q学習エージェントの訓練と評価を行うメインスクリプト。
-   `rl_test.py`: `nasim`環境の状態空間や観測データの形式を確認するためのテスト用スクリプト。
-   `requirements.txt`: プロジェクトの依存ライブラリ。

## 5. 実行方法

### 1. 環境のセットアップ

以下のコマンドで、必要なライブラリをインストールします。

```bash
pip install -r requirements.txt
```

### 2. 学習の実行

以下のコマンドで、`tiny`環境におけるQ学習エージェントの学習を開始します。

```bash
python rl_1.py
```

実行が完了すると、学習の進捗と、学習済みエージェントによるテストプレイの結果が出力されます。

## 6. 今後の展望

-   **より高度なRLアルゴリズムの導入**:
    -   DQN (Deep Q-Network) などの深層強化学習を導入し、大規模で複雑なネットワーク状態に対応。
-   **多様なシナリオでの実験**:
    -   `nasim`で定義されている他のベンチマークシナリオ（`small`, `medium`など）や、独自のカスタムシナリオでエージェントの汎化性能を評価。
-   **防御エージェントの実装**:
    -   攻撃エージェントと対峙する防御エージェントを学習させ、マルチエージェント環境での攻防シミュレーションを実施。
-   **部分観測環境への挑戦**:
    -   より現実的なシナリオである部分観測環境（`fully_obs=False`）でのエージェント学習。
